
---
title: "STAT340 HW07: Estimation II"
author: Jeffrey Hui
date: "October 2022"
output: html_document
---

***

TODO: If you worked with any other students on this homework, please list their names and NetIDs here.

***

## Instructions

Update the "author" and "date" fields in the header and
complete the exercises below.
Knit the document, and submit **both the HTML and RMD** files to Canvas.

__Due date:__ November 3, 2022 at 11:59pm.

---

This homework will review our discussion of estimation and confidence intervals from this week's lectures.

## 1) Comparing CLT- and simulation-based CIs

In lecture, we saw two different approaches to building confidence intervals.

* One, hopefully familiar to you from previous introductory courses, was based on the CLT, namely the fact that the sample mean is (approximately) normal about the population mean after appropriate rescaling (i.e., dividing by the standard deviation).
* The other, probably new to you, was simulation-based.
We estimate our parameter(s) of interest, then generate new data based on those (estimated) parameters.
We can then use that data to estimate the parameter(s) *again*, and the behavior of those "fake data" estimates can tell us about the behavior of our estimator with respect to the true parameter(s).

One thing we didn't discuss in lecture is how these two methods compare (except to say that... it depends).

This problem is fairly open-ended. Your job is to set up an experiment comparing these two different confidence interval methods.

### Part a: The data generating model

First things first, we need to generate data.
Define a function `generate_data`, that takes two arguments, `lambda` and `nsamp`, where `lambda` is the parameter of a Poisson and `nsamp` is a positive integer specifying a number of samples.
Your function should return a vector of length `nsamp`, whose entries are drawn iid from Poisson with parameter given by `lambda`.
`lambda` should default to 1.

```{r}
generate_data <- function( nsamp, lambda=1 ) {
  return (rpois(nsamp, lambda))
}
```

### Part b: fitting the data

Our simulation-based method requires that we have an estimate of the mean, and our CLT-based method requires that we have estimates of the mean and variance.
Write functions `estimate_pois_mean` and `estimate_pois_var` to estimate both the mean and variance of the Poisson given the output of `generate_data`.

__Hint:__ the Poisson has the convenient property that the expectation and variance are both equal to $\lambda$, so these can be the same function-- both can just return the sample mean, if you want, but you can also use the sample variance. Your choice!

```{r}
estimate_pois_mean <- function( data ) {
  return (mean(data))
}

estimate_pois_var <- function( data ) {
  return( estimate_pois_mean( data ))
}

```

### Part c: CLT-based confidence intervals

Implement a function `CLT_CI` that takes arguments `data` (the vector of data generated by `generate_data`) and `alpha` (a numeric between 0 and 1, i.e, a probability, which should default to $0.05$), and returns a confidence interval (i.e., a vector of length 2 whose first entry is the left end of the interval and whose second entry is the right end of the interval).

This confidence interval should be based on our CLT-based approximation. That is, you should use the fact that
$$
  \frac{ \bar{X} - \lambda }{ \sqrt{ (\operatorname{Var X_1})/n }}
$$
is approximately normal.

Use your `estimate_pois_var` function to estimate the variance term in the denominator (ignore the fact that this is equal to $\lambda$), use `estimate_pois_mean` to get $\bar{X}$ and follow the basic outline from lecture to create a $(1-\alpha)*100$-percent *two-sided* confidence interval (if "two-sided" is not familiar to you, no worries-- just follow the recipe from lecture and you'll be fine!).

```{r}
CLT_CI <- function( data, alpha=0.05) {
  # TODO: code here.
  
  # Step 1. use data to get Xbar and the variance estimate.
  xbar <- estimate_pois_mean(data)
  varbar <- estimate_pois_var(data)
  n <- length(data)
  
  # Step 2. Use the estimated variance and choice of alpha to construct
  # a two-sided confidence interval for lambda
  # (sorry, Z-scores are going to show up here, but only briefly!)
  quant <- qnorm(1 - alpha / 2)
  
  # Return the CI. Be careful-- depending on how you got your CI, it
  # might not be a simple vector (e.g., it might have a header).
  # Feel free to use the lecture code for reference.
  return (c(xbar - quant * varbar / sqrt(n), xbar + quant * varbar / sqrt(n)))
  
}
```

### Part d: simulation-based confidence intervals

Implement a function `simulation_CI` that takes arguments `data` (the vector of data generated by `generate_data`) and `alpha` (a numeric between o and 1, i.e, a probability, which should default to $0.05$), and returns a confidence interval (i.e., a vector of length 2 whose first entry is the left end of the interval and whose second entry is the right end of the interval).

Your function should construct the confidence interval according to the simulation-based approach discussed in class.
You are free to adapt the code from lecture in your code.
You may pick the number of Monte Carlo iterates (i.e., replicates) as you wish, but we would recommend setting it to be at least a few hundred.

```{r}
simulation_CI <- function( data, alpha=0.05) {
  lambdahat <- estimate_pois_mean(data)
  n_sim <- 1e3
  lambda_stars <- rep(0, n_sim)
  for (i in 1:n_sim) {
    lambda_stars[i] <- mean(rpois(100, lambda = lambdahat))
  }

  # Return a vector c(L, U) with L the left end and U the right ("upper") end of the CI.
  return (c(quantile(lambda_stars, alpha / 2)[[1]], quantile(lambda_stars, 1 - alpha / 2)[[1]]))
}

```

### Part e: Comparing Confidence Intervals

So, now you've implemented two different confidence intervals.
Let's compare them.

First of all, we need to be able to check if our CI "caught" the true parameter or not.

Write a function `contained` that takes a CI (i.e., a two-vector with first entry smaller than the second) and a number (the true value of the parameter) that returns TRUE if the true value of the parameter is inside the interval and FALSE otherwise.

```{r}
contained <- function( myCI, trueparam ) {
  return (myCI[1] < trueparam & trueparam < myCI[2])
}

```

What does it mean for one confidence interval to be better than another?
We define the *coverage* of a confidence interval to be the probability that the true parameter lies in the interval.

Ideally, a $1-\alpha$ confidence interval has coverage $1-\alpha$. Simple!

The problem arises from approximation-- our CLT- and simulation-based CIs are based on approximations, and so their coverage will not be exactly $1-\alpha$.

Implement a function called `estimate_coverage`, whose arguments are given in the code block below, and which returns an estimate of the coverage (i.e., a number between 0 and 1).

* `CI_fn` is a *function* (in our code, this will be either `CLT_CI` or `simulation_CI`). Yes, functions can be arguments to other functions in R! Note that this argument will just be a function. *Not a string.*
* `NMC` is a positive integer, the number of replicates to run.
* `nsamp` is a positive integer, the number of samples.
* `lambdatrue` is a positive real, the parameter of the Poisson that we will use to generate our data. It should default to 1.
* `alpha` is a numeric between 0 and 1. `1-alpha` will be the (target) level of our CI.

```{r}
estimate_coverage <- function( CI_fn, NMC, nsamp, lambdatrue=1, alpha=0.05 ) {
  coverages <- 0; # Count how often the CI "catches" lambdatrue
  
  for (i in 1:NMC ) {
    # Generate data: nsamp draws from Poisson( lambdatrue )
    data <- rpois(n=nsamp, lambda=lambdatrue);
    # Construct a confidence interval from it using the given CI function
    interval <- CI_fn(data, alpha)
    # if lambdatrue is in the CI, increment coverages.
    if (contained(interval, lambdatrue)) {
      coverages <- coverages + 1
    }
  }
  
  return( coverages/NMC )
}

```

### Part f: exploring

Wow, that was a lot of coding! Here comes the payoff!

Use the code above to estimate the coverage of your two CI methods.
Try changing the different arguments-- $\lambda$, $\alpha$, `nsamp`, etc.
Coverages of the two methods *should* both be close to $1-\alpha$, whatever you chose $\alpha$ to be, but you will likely find that they tend to follow patterns, consistently either over- or under-shooting the target.

Take some time to explore these patterns. What do you see?
There are no right or wrong answers, here.
We are just asking you to see how these two different CI methods behave in different situations.

An important point that we alluded to above is that in the Poisson, the mean and variance are both just $\lambda$.
This is actually part of why you may have noticed some weird behavior in your experiments above.
Unfortunately, a thorough explanation of that weird behavior will have to wait for your later theory courses.

```{r}
nmc <- 100
nsamp <- 400
l <- 1
al <- .05
estimate_coverage(CLT_CI, nmc, nsamp, lambdatrue = l, alpha = al)
estimate_coverage(simulation_CI, nmc, nsamp, lambdatrue = l, alpha = al)
```

***

I observed that the simulation method's coverage is affected by the parameter `nsamp`, whereas the CLT method is fairly stable with its coverage (around 0.95 for alpha = 0.05) no matter how I change the `nsamp` parameter. The simulation coverage will tend to go up with larger `nsamp` and vice versa.

***

## 2) The infamous mule kick data

The file `mule_kicks.csv`, available for download (here)[https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv], contains a simplified version of a very famous data set.
The data consists of the number of soldiers killed by being kicked by mules or horses each year in a number of different companies in the Prussian army near the end of the 19th century.

This may seem at first to be a very silly thing to collect data about, but it is a very interesting thing to look at if you are interested in rare events.
Deaths by horse kick were rare events that occurred independently of one another, and thus it is precisely the kind of process that we might expect to obey a Poisson distribution.

Download the data and read it into R by running
```{r}
download.file('https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv', destfile='mule_kicks.csv');
mule_kicks <- read.csv('mule_kicks.csv', header=TRUE);

head(mule_kicks);
```

`mule_kicks` contains a single column, called `deaths`.
Each entry is the number of soldiers killed in one corps of the Prussian army in one year.
There are 14 corps in the data set, studied over 20 years, for a total of 280 death counts.

### Part a: estimating the Poisson rate

Assuming that the mule kicks data follows a Poisson distribution, produce a point estimate for the rate parameter $\lambda$.
There are no strictly right or wrong answers, here, though there are certainly better or worse ones.

```{r}
#TODO: estimate the rate parameter.

lambdahat <- mean(mule_kicks$deaths); # TODO: write code; store your estimate in lambdahat.
lambdahat
```

### Part b: constructing a CI

Using everything you know (Monte Carlo, CLT, etc.), construct a confidence interval for the rate parameter $\lambda$.
Explain in reasonable detail what you are doing and why you are constructing the confidence interval in this way (a few sentences is fine!).

```{r}

simulate <- function( lambdahat, alpha=0.05) {
  n_sim <- 2e4
  lambda_stars <- rep(0, n_sim)
  for (i in 1:n_sim) {
    lambda_stars[i] <- mean(rpois(1000, lambda = lambdahat))
  }

  # Return a vector c(L, U) with L the left end and U the right ("upper") end of the CI.
  return (c(quantile(lambda_stars, alpha / 2)[[1]], quantile(lambda_stars, 1 - alpha / 2)[[1]]))
}

simulate(lambdahat, alpha = 0.05)
```

***

First I estimate the lambda by taking the mean of the data. Then I constructed the confidence interval by fixing my lambda estimate and simulating the confidence interval. For each iteration I computed the point estimate for the generated data, and I looked at the quantiles at the end based on the alpha value.

***

### Part c: assessing a model

Here's a slightly more open-ended question.
We *assumed* that the data followed a Poisson distribution.
This may or may not be a reasonable assumption.
Use any and all tools that you know to assess (either with code or simply in words) how reasonable or unreasonable this assumption is.

Once again, there are no strictly right or wrong answers here.
Explain and defend your decisions and thought processes in a reasonable way and you will receive full credit.

***

I do not think that it is a reasonable assumption. Although the Poisson distribution is good for modeling the flow in a unit of time, the time span in this example is over 20 years, which increases the factors that might affect this model. For example, the rate might not be constant throughout 20 years and would lead to significant change in the estimation.

***

